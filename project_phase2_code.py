# -*- coding: utf-8 -*-
"""project phase2 code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K4U4wqNg7zBrzJbZ238jWjiaSZf6Ad66

Upload the Dataset
"""

from google.colab import files
uploaded = files.upload()

"""Load the Dataset"""

import pandas as pd
# Read the dataset
df = pd.read_csv('job_descriptions.csv')

"""Data Exploration"""

# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("job_descriptions.csv")

# Basic info
print("üìå Shape of the dataset:", df.shape)
print("\nüìå Column names:\n", df.columns)
print("\nüìå Data types:\n", df.dtypes)

# Preview data
print("\nüìå First 5 rows:\n", df.head())

# Missing values
print("\nüìå Missing values per column:\n", df.isnull().sum())

# Visualize missing data
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, cmap="viridis")
plt.title("Missing Values Heatmap")
plt.tight_layout()
plt.savefig("missing_data_heatmap.png")
plt.show()

# Summary statistics for object-type columns
for col in df.columns:
    if df[col].dtype == 'object':
        print(f"\nüîπ Value counts for '{col}':")
        print(df[col].value_counts().head(10))

# Top 10 Job Titles
plt.figure(figsize=(12, 6))
df['Job Title'].value_counts().head(10).plot(kind='barh', color='skyblue')
plt.title("Top 10 Job Titles")
plt.xlabel("Frequency")
plt.ylabel("Job Title")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig("top_10_job_titles.png")
plt.show()

# Experience level distribution (if column exists and is structured)
if 'Years of Experience' in df.columns:
    plt.figure(figsize=(10, 5))
    df['Years of Experience'].dropna().value_counts().sort_index().plot(kind='bar', color='salmon')
    plt.title("Years of Experience Distribution")
    plt.xlabel("Experience")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

"""Check for Missing Values and Duplicates"""

import pandas as pd

# Load the dataset
df = pd.read_csv("job_descriptions.csv")

# === Missing Values ===
print("üîç Missing Values per Column:\n")
missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])

# Percentage of missing values (optional)
print("\nüìä Percentage of Missing Values:\n")
print((missing_values / len(df) * 100).round(2))

# === Duplicate Rows ===
print("\nüîÅ Checking for Duplicate Rows...\n")
duplicate_rows = df.duplicated()
print(f"Total duplicate rows: {duplicate_rows.sum()}")

# Optionally show duplicate rows
if duplicate_rows.sum() > 0:
    print("\nüßæ Duplicate Entries:\n")
    print(df[duplicate_rows])

# Drop duplicates if needed (optional)
# df = df.drop_duplicates()
# print("‚úÖ Duplicates dropped. New shape:", df.shape)

"""Visualize a Few Features"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv("job_descriptions.csv")

# Set Seaborn style
sns.set(style="whitegrid")

# =======================
# 1. Top 10 Job Titles
# =======================
plt.figure(figsize=(12, 6))
df['Job Title'].value_counts().head(10).plot(kind='barh', color='skyblue')
plt.title("Top 10 Job Titles")
plt.xlabel("Frequency")
plt.ylabel("Job Title")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# =======================
# 2. Job Levels Distribution
# =======================
if 'Level' in df.columns:
    plt.figure(figsize=(10, 5))
    sns.countplot(data=df, y='Level', order=df['Level'].value_counts().index, palette='Set2')
    plt.title("Job Level Distribution")
    plt.xlabel("Count")
    plt.ylabel("Level")
    plt.tight_layout()
    plt.show()

# =======================
# 3. Country Distribution
# =======================
if 'Country' in df.columns:
    plt.figure(figsize=(10, 5))
    df['Country'].value_counts().head(10).plot(kind='bar', color='orange')
    plt.title("Top 10 Countries Posting Jobs")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# =======================
# 4. Years of Experience
# =======================
if 'Years of Experience' in df.columns:
    plt.figure(figsize=(10, 5))
    df['Years of Experience'].dropna().value_counts().sort_index().plot(kind='bar', color='purple')
    plt.title("Years of Experience Distribution")
    plt.xlabel("Experience")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

"""Identify Target and Features"""

import pandas as pd

# Load dataset
df = pd.read_csv("job_descriptions.csv")

# Show all column names
print("üìå Columns in Dataset:")
print(df.columns.tolist())

# Optionally: Automatically suggest target based on common keywords
target_keywords = ['target', 'label', 'category', 'output', 'y']
suggested_targets = [col for col in df.columns if any(key in col.lower() for key in target_keywords)]

if suggested_targets:
    print("\nüß≠ Suggested Target Column(s):", suggested_targets)
else:
    print("\n‚ö†Ô∏è No obvious target column found. Please manually inspect and choose one.")

# Manual input (example: change to your actual target)
target_column = 'Job Title'  # <-- Update this to your actual target if known

# Define features as all other columns except the target
feature_columns = [col for col in df.columns if col != target_column]

print(f"\nüéØ Target: {target_column}")
print(f"\nüßÆ Features ({len(feature_columns)}): {feature_columns}")

"""Convert Categorical Columns to Numerical"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Sample dataset
data = {
    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red'],
    'Size': ['S', 'M', 'L', 'XL', 'M'],
    'Price': [100, 150, 200, 130, 170]
}

df = pd.DataFrame(data)

print("Original DataFrame:")
print(df)

# Encode all object (categorical) columns using LabelEncoder
label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le  # Save encoder if you need to reverse transform

print("\nDataFrame after encoding categorical columns:")
print(df)

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Sample data
data = {
    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red'],
    'Size': ['S', 'M', 'L', 'XL', 'M']
}

df = pd.DataFrame(data)

# Create the encoder
encoder = OneHotEncoder(sparse_output=False)  # Use sparse_output=False instead of sparse=False (for newer sklearn)

# Fit and transform the data
encoded_array = encoder.fit_transform(df)

# Create a DataFrame with encoded feature names
encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(df.columns))

print("One-Hot Encoded DataFrame using sklearn:")
print(encoded_df)

"""One-Hot Encoding"""

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Sample data
data = {
    'Age': [25, 32, 47, 51, 62],
    'Salary': [50000, 60000, 80000, 72000, 90000]
}

df = pd.DataFrame(data)

print("Original DataFrame:")
print(df)

# --- Standard Scaling (Z-score normalization) ---
standard_scaler = StandardScaler()
df_standard_scaled = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)

print("\nStandard Scaled Data (mean=0, std=1):")
print(df_standard_scaled)

# --- Min-Max Scaling (0 to 1 range) ---
minmax_scaler = MinMaxScaler()
df_minmax_scaled = pd.DataFrame(minmax_scaler.fit_transform(df), columns=df.columns)

print("\nMin-Max Scaled Data (range 0 to 1):")
print(df_minmax_scaled)

"""Feature Scaling"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Sample dataset
data = {
    'Age': [25, 32, 47, 51, 62],
    'Salary': [50000, 60000, 80000, 72000, 90000],
    'Purchased': [0, 1, 0, 1, 1]  # Target variable
}

df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['Age', 'Salary']]
y = df['Purchased']

# Split the data: 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Output
print("X_train:\n", X_train)
print("\nX_test:\n", X_test)
print("\ny_train:\n", y_train)
print("\ny_test:\n", y_test)

"""Train-Test Split"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Sample dataset
data = {
    'Age': [25, 32, 47, 51, 62],
    'Salary': [50000, 60000, 80000, 72000, 90000],
    'Purchased': [0, 1, 0, 1, 1]  # Target variable
}

df = pd.DataFrame(data)

# Features and target
X = df[['Age', 'Salary']]
y = df['Purchased']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model building: Logistic Regression
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

"""Model Building"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Sample dataset
data = {
    'Age': [25, 32, 47, 51, 62],
    'Salary': [50000, 60000, 80000, 72000, 90000],
    'Purchased': [0, 1, 0, 1, 1]  # Target variable
}

df = pd.DataFrame(data)

# Features and target
X = df[['Age', 'Salary']]
y = df['Purchased']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation metrics
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("Precision Score:", precision_score(y_test, y_pred))
print("Recall Score:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""Evaluation"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Sample training data
data = {
    'Age': [25, 32, 47, 51, 62],
    'Salary': [50000, 60000, 80000, 72000, 90000],
    'Purchased': [0, 1, 0, 1, 1]  # Target variable
}

df = pd.DataFrame(data)

# Features and target
X = df[['Age', 'Salary']]
y = df['Purchased']

# Train-test split (not needed just for predictions but included for realism)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model training
model = LogisticRegression()
model.fit(X_train, y_train)

# ‚úÖ NEW input data for prediction
new_input = pd.DataFrame({
    'Age': [45, 30],
    'Salary': [70000, 55000]
})

# Make predictions
predictions = model.predict(new_input)

# Output
for i, pred in enumerate(predictions):
    print(f"Input {i+1}: Age={new_input.iloc[i]['Age']}, Salary={new_input.iloc[i]['Salary']} ‚Üí Predicted Purchased = {pred}")

"""Make Predictions from New Input"""

import pandas as pd

# Step 1: Raw data (e.g., list of dictionaries)
data = [
    {'Name': 'Alice', 'Gender': 'Female', 'City': 'New York'},
    {'Name': 'Bob', 'Gender': 'Male', 'City': 'Paris'},
    {'Name': 'Charlie', 'Gender': 'Male', 'City': 'London'}
]

# Step 2: Convert to DataFrame
df = pd.DataFrame(data)
print("Original DataFrame:\n", df)

# Step 3: One-Hot Encode categorical columns (excluding 'Name' here)
df_encoded = pd.get_dummies(df, columns=['Gender', 'City'])

print("\nEncoded DataFrame:\n", df_encoded)

"""Convert to DataFrame and Encode"""

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Sample dataset
data = {
    'StudyHours': [2, 4, 6, 8, 10],
    'PastGrade': [60, 65, 70, 85, 90],
    'Attendance': [70, 75, 80, 90, 95],
    'FinalGrade': [65, 68, 75, 88, 92]  # Target
}

# Step 1: Convert to DataFrame
df = pd.DataFrame(data)

# Step 2: Define features (X) and target (y)
X = df[['StudyHours', 'PastGrade', 'Attendance']]
y = df['FinalGrade']

# Step 3: Split data for training/testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 5: Evaluate model
y_pred = model.predict(X_test)
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))

# Step 6: Predict Final Grade for new input
new_student = pd.DataFrame({
    'StudyHours': [7],
    'PastGrade': [75],
    'Attendance': [85]
})

predicted_grade = model.predict(new_student)
print(f"Predicted Final Grade: {predicted_grade[0]:.2f}")

"""Predict the Final Grade"""

import pandas as pd
from sklearn.linear_model import LinearRegression

# Sample training data
data = {
    'StudyHours': [2, 4, 6, 8, 10],
    'PastGrade': [60, 65, 70, 85, 90],
    'Attendance': [70, 75, 80, 90, 95],
    'FinalGrade': [65, 68, 75, 88, 92]
}

# Step 1: Convert to DataFrame
df = pd.DataFrame(data)

# Step 2: Split features and target
X = df[['StudyHours', 'PastGrade', 'Attendance']]
y = df['FinalGrade']

# Step 3: Train the model
model = LinearRegression()
model.fit(X, y)

# ‚úÖ Step 4: Define the prediction function
def predict_final_grade(study_hours, past_grade, attendance):
    input_data = pd.DataFrame([[study_hours, past_grade, attendance]],
                              columns=['StudyHours', 'PastGrade', 'Attendance'])
    prediction = model.predict(input_data)[0]
    return round(prediction, 2)

# Step 5: Example usage
result = predict_final_grade(7, 72, 88)
print(f"Predicted Final Grade: {result}")

pip install gradio pandas

"""Deployment-Building an Interactive App"""

import pandas as pd
import gradio as gr

# Load your CSV file
df = pd.read_csv("job_descriptions.csv")

# Function to search jobs based on keyword
def search_jobs(keyword):
    if not keyword.strip():
        return "Please enter a keyword to search."

    keyword_lower = keyword.lower()

    # Filter DataFrame
    filtered = df[
        df['Job Title'].str.lower().str.contains(keyword_lower, na=False) |
        df['Job Description'].str.lower().str.contains(keyword_lower, na=False) |
        df['skills'].str.lower().str.contains(keyword_lower, na=False)
    ]

    # Return top matches
    display_columns = ['Job Title', 'Company', 'location', 'Work Type', 'Salary Range', 'Job Description']
    if filtered.empty:
        return "No matching jobs found."

    return filtered[display_columns].head(10).to_markdown(index=False)

# Gradio interface
interface = gr.Interface(
    fn=search_jobs,
    inputs=gr.Textbox(label="Enter keyword (e.g., Python, Manager, Marketing)"),
    outputs=gr.Markdown(label="Matching Jobs"),
    title="üîç Job Description Explorer",
    description="Search job listings by title, description, or skills."
)

interface.launch()